{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cardiovascular-breed",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/howesa/CHI22-CogMod-Tutorial/blob/main/04-Go_Nogo/04_Go_Nogo.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e1ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the files to local.\n",
    "# Please note that the go / no go model we are using here is still being developed,\n",
    "# and should not yet be adapted for other work!\n",
    "! wget https://raw.githubusercontent.com/howesa/CHI22-CogMod-Tutorial/main/04-Go_Nogo/animate_trace.py\n",
    "! wget https://raw.githubusercontent.com/howesa/CHI22-CogMod-Tutorial/main/04-Go_Nogo/driver_agent_physics.py\n",
    "! wget https://raw.githubusercontent.com/howesa/CHI22-CogMod-Tutorial/main/04-Go_Nogo/go_nogo.py\n",
    "! wget https://raw.githubusercontent.com/howesa/CHI22-CogMod-Tutorial/main/04-Go_Nogo/physics_env.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02706422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required libraries.\n",
    "! pip install --pre -U stable_baselines3\n",
    "! pip install gym\n",
    "! pip install shimmy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50888742",
   "metadata": {},
   "source": [
    "# Module 4: Building a Model\n",
    "\n",
    "In this module, we take a step-by-step walktrough of how to create a computational rational (CR) model using deep reinforcement learning (RL). This notebook does not cover the full workflow of CR modeling, which is long and detailed. It can be found here, make sure to follow it when creating your own models. For the purpose of this notebook, the simplified workflow looks like this:\n",
    "\n",
    "1. Define the goals.\n",
    "\n",
    "2. Define the environment.\n",
    "\n",
    "3. Define the cognitive limitations.\n",
    "\n",
    "4. Derive the optimal behavior.\n",
    "\n",
    "5. Inspect model validity.\n",
    "\n",
    "The model will be defined according to the standard CR flow of information.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/howesa/CHI22-CogMod-Tutorial/main/04-Go_Nogo/corati_model.png\" alt=\"Corati Modeling\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac368997",
   "metadata": {},
   "source": [
    "# 1. Define the agent's goals\n",
    "\n",
    "The task explored in this notebook is a fairly simple one: in a junction, when turning against the oncoming traffic, the driver needs to decide if they can go, or if they need to wait for an oncoming car before they can cross. In left-handed traffic, this means that the agent driver is turning right. Here is the illustration:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/howesa/CHI22-CogMod-Tutorial/main/04-Go_Nogo/go_nogo_task.png\" alt=\"Go / No go task\" width=\"300\">\n",
    "\n",
    "The agent (the yellow car) has two main goals:\n",
    "\n",
    "1. Proceed to the destination by turning left.\n",
    "\n",
    "2. Drive safely, avoiding collisions.\n",
    "\n",
    "First, we need to analyze these goals. First, the agent wants to drive efficiently and not get stuck on the road for too long. They probably want to get to their destination, and also they would be blocking the traffic behind them if they wait unreasonably long. So we can analyse these goals into a reward function.\n",
    "\n",
    "1. When the agent turns left successfully, there is a positive reward.\n",
    "2. For this positive reward, there is a penalty of time spent waiting.\n",
    "3. For a collision, there is a negative reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a685485",
   "metadata": {},
   "source": [
    "# 2. Define the task environment\n",
    "\n",
    "For simplicity, the environment has only two cars, in a 2-dimensional environment: the agent's car and the oncoming car. The cars are particles that have their individual `(x,y)` coordinates. The agent's car is stationary until the agent decides to turn left. The oncoming car has a variable position, which moves along the y-axis of the environment according to its velocity, which is fixed constant. A collision is detected if the vehicles are too close to each other regardless of their velocities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e8921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define an environment and an agent and see how it works.\n",
    "import physics_env\n",
    "import driver_agent_physics\n",
    "\n",
    "e = physics_env.physics_env()\n",
    "agent = driver_agent_physics.driver_agent_physics(e, observation_var = 0) # a full observer: no noise\n",
    "agent.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a692d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's wait for 20 ticks, then go.\n",
    "from IPython.display import HTML, display\n",
    "import animate_trace\n",
    "\n",
    "agent.reset()\n",
    "agent.env.save_trace = True\n",
    "for i in range(20):\n",
    "    agent.step(0)\n",
    "agent.step(1)\n",
    "agent.env.save_trace = False\n",
    "\n",
    "HTML(animate_trace.animate_trace(agent.env.trace, get_anim = True, x_lim = [-50,50], y_lim = [10,-80]).to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed88fdb",
   "metadata": {},
   "source": [
    "# 3. Define the relevant cognitive bounds of the agent\n",
    "\n",
    "The agent must make a decision to go (turn left) or wait. It bases this decision on a noisy observation of the oncoming car's distance to the agent's car. If the distance is long enough, the agent can go and save time. If the car is too close, the agent must wait for it to pass to avoid a collision.\n",
    "\n",
    "For modeling noisy observation, we will be using the formula from Markkula, et al. (2022). Explaining human interactions on the road requires large-scale integration of psychological theory. https://psyarxiv.com/hdxbs/\n",
    "\n",
    "$ \\hat{D} = D_{oth} \\cdot \\left(1 - \\frac{h}{D \\cdot \\tan \\left(\\arctan \\left(\\frac{h}{D}\\right) + \\sigma \\right)}\\right) $,\n",
    "\n",
    "where $ \\hat{D} $ is a noisy observation of distance, $D_{oth}$ is the oncoming car's longitudinal distance to the crossing point, and $h$ is the observer's eye height over ground (1.5m). The important parameter here is $\\sigma$, which describes how noisy the observation is.\n",
    "\n",
    "The noisy observation is not used directly, but via Bayesian filtering. This considers prior belief about the distance and integrates the new observation with it. Additionally, we represent the uncertainty associated with the belief.\n",
    "\n",
    "Before deriving the optimal behavior, we want to establish what hypotheses our model in fact makes, so that we can then assess the plausibility of model predictions against them. While modeling is virtually always interactive and the hypotheses might develop during iteration, but it is important to have a strong initial assumption about how our model will behave. Here are initial proposals, which would then be turned into exact, testable hypotheses:\n",
    "\n",
    "P1. For a non-noisy (full) observer, there is a precise distance: if the other car is farther than this distance, the agent decides to go. If the other car is closer, the agent chooses to wait. If the car has passed, the agent always goes.\n",
    "\n",
    "P2. For a noisy (partial) observer, the decision to go is more conservative and uncertain: the distance that determines the go / no go decision is larger, and due to noisy estimates, the choice is probabilistic given distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b960cbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a theoretical illustration of our proposals.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "x1 = np.linspace(-1, 0, 100)\n",
    "x2 = np.linspace(-1, 0, 100)\n",
    "y1 = np.zeros(100)\n",
    "y1[50:] = 1\n",
    "y2 = 1 / (1 + np.exp(-10*(x2+0.5)))\n",
    "plt.plot(x1, y1, label='Full Observer', linewidth = 2)\n",
    "plt.plot(x2, y2, label='Noisy Observer', linewidth = 2)\n",
    "plt.ylim(0, 1.01)\n",
    "plt.xlabel('Distance', fontsize=14)\n",
    "plt.ylabel('Probability of wait', fontsize=14)\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8610046c",
   "metadata": {},
   "source": [
    "## How does the noisy observation happen?\n",
    "\n",
    "The observation of the approaching vehicle distance is noisy. In practice, we simulate noisy observation by taking the true observation and then adding noise from a normal distribution, according to some parameter sigma. However, instead of using the returned noisy value as such, we assume that the human visual system exploits the fact that each noisy value is based on the true value, and the noise comes from a known distribution. We can then use a Kalman filter to approximate the true state. With more observations, the approximation becomes better. In our simulation, this is confounded by the formula above, which states that observations get better as the distance shortens. Let's investigate this behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994429e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Here is the formula for making an observation, given distance.\n",
    "def noisy_observation(D):\n",
    "    # D = distance in meters\n",
    "    d_oth = D+2 # crossing point distance, this is a crude approximation to simplify our exercise\n",
    "    h = 1.5 # eye height\n",
    "    observation_var = 0.1 # this is the sigma\n",
    "    distance_var = d_oth * (1 - h / (D*math.tan(math.atan(h/D) + observation_var)))\n",
    "    observed_distance = np.random.normal(D, distance_var)\n",
    "    return observed_distance, distance_var\n",
    "\n",
    "\n",
    "# Let's see what kinds of observations we make from the true distance of 50 m\n",
    "for i in range(10):\n",
    "    print(noisy_observation(50)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db93f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, that's not a very reliable observation! Here is the Kalman filter, let's see how it helps.\n",
    "def kalman_update(prior_mean, prior_var, observation, s):\n",
    "        observation_gain = 1\n",
    "        kalman_gain = prior_var * observation_gain / (prior_var * observation_gain**2 + s**2)\n",
    "        posterior_mean = prior_mean + kalman_gain * (observation - prior_mean)\n",
    "        posterior_var = (1 - kalman_gain * observation_gain) * prior_var\n",
    "\n",
    "        return posterior_mean, posterior_var\n",
    "    \n",
    "# Then, let's update both mean and variance estimates multiple times\n",
    "distance_prior = 100 # set an uninformed prior\n",
    "distance_var_prior = 1000 # basically a uniform prior\n",
    "for i in range(20):\n",
    "    d, s = noisy_observation(50)\n",
    "    distance_post, distance_var_post = kalman_update(distance_prior, distance_var_prior, d, s)\n",
    "    # Store result\n",
    "    print(distance_post)\n",
    "    # Set the posterior as the new prior\n",
    "    distance_prior = distance_post\n",
    "    distance_var_prior = distance_var_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47023b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we visualize it? Let's assume a constant velocity of 0.5 m per observational \"tick\".\n",
    "# Assuming one tick is 0.05s, this is 10m/s = 36km/h\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "true_d = 50\n",
    "distance_prior = 100 # set an uninformed prior\n",
    "distance_var_prior = 1000 # basically a uniform prior\n",
    "velocity = 0.5\n",
    "data = []\n",
    "for i in range(100):\n",
    "    d, s = noisy_observation(true_d)\n",
    "    distance_post, distance_var_post = kalman_update(distance_prior, distance_var_prior, d, s)\n",
    "    # Store result\n",
    "    data.append([true_d, d, distance_post])\n",
    "    # Set the posterior as the new prior\n",
    "    distance_prior = distance_post\n",
    "    distance_var_prior = distance_var_post\n",
    "    # \"Move\" the approaching vehicle\n",
    "    true_d -= velocity\n",
    "    \n",
    "# Make the plot\n",
    "true_distance, observed_distance, estimated_distance = zip(*data)\n",
    "index = np.array(range(len(data))) / 20 # make into seconds\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.plot(index, true_distance, label='True Distance', linestyle='-', marker='o')\n",
    "plt.plot(index, observed_distance, label='Observed Distance', linestyle='-', marker='o')\n",
    "plt.plot(index, estimated_distance, label='Estimated Distance', linestyle='-', marker='o')\n",
    "\n",
    "plt.xlabel('Time (s)', fontsize = 14)\n",
    "plt.ylabel('Distance', fontsize = 14)\n",
    "plt.legend(loc='best', fontsize = 14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982fa065",
   "metadata": {},
   "source": [
    "# 4. Derive optimal policy\n",
    "\n",
    "For establishing the (bounded) optimal policy for the ideal and noisy observer agents, we will be using Proximal Policy Optimization, which is an on-policy deep RL algorithm. This notebook uses the OpenAI Stable Baselines implementation, but there are others as well. https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html\n",
    "\n",
    "Before we can derive the optimal policy, the environments (both internal and external) need to be modeled using Markov Decision Process (MDP, or in our case, due to partial observability, we are defining a POMDP). We won't go into the details of how the external driving environment is modeled, but it is a simple stepwise simulator, that for each time step (0.05 seconds by default), \"ticks\" the environment forward by moving the upcoming car's y-position according to its velocity, and in case the agent decides to go, moves it towards the side road along a predefined trajectory, in a constant velocity as well.\n",
    "\n",
    "For the internal environment, we need to model the belief update for distance of the oncoming car, given the equation and filtering that were defined above. All relevant information must be represented as the agent's belief such that we can pass that, along with the reward signal, to the RL agent for learning the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b9e221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the action and observation spaces of the agent\n",
    "\n",
    "from gym.spaces import Discrete, Dict, Box\n",
    "\n",
    "action_space = Discrete(2)        \n",
    "\n",
    "# Note that all observatons are normalized between 0 and 1.\n",
    "observation_space = Dict(\n",
    "    spaces = {\n",
    "        \"distance\": Box(0, 1, (1,)),\n",
    "        \"passed\": Box(0, 1, (1,)),\n",
    "        \"distance_var\": Box(0, 1, (1,)),\n",
    "        \"speed\": Box(0, 1, (1,)),\n",
    "        \"acceleration\": Box(0, 1, (1,)),\n",
    "        \"ticks\": Box(0, 1, (1,))\n",
    "    })\n",
    "\n",
    "# Sample the action space:\n",
    "print(\"A small sample of actions:\", action_space.sample(), action_space.sample(), action_space.sample())\n",
    "print(\"One sample of observation space:\", observation_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae9030e",
   "metadata": {},
   "source": [
    "So, the environment simulation works as intended. Let's now take a look at the reward function. Remember our definition:\n",
    "\n",
    "1. When the agent turns left successfully, there is a positive reward.\n",
    "2. For this positive reward, there is a penalty of time spent waiting.\n",
    "3. For a collision, there is a negative reward.\n",
    "\n",
    "We will establish the reward function along with one step of the model to see what actually happens when we step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8227436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(self, action):\n",
    "    self.reward = 0\n",
    "    # action: no go\n",
    "    if action == 0:\n",
    "        self.env.tick()\n",
    "        self.ticks += 1\n",
    "        # break if nothing ever happens\n",
    "        if self.ticks > self.max_ticks:\n",
    "            self.reward = -10\n",
    "            self.done = True\n",
    "        if self.env.get_distance() > self.max_distance:\n",
    "            self.reward = -10\n",
    "            self.done = True                \n",
    "    # action: go\n",
    "    if action == 1:\n",
    "        # Did we wait for the other car before going?\n",
    "        if self.env.veh2_turn_pos[1] < self.env.veh1_straight_pos[1]:\n",
    "            self.waited_before_go = True\n",
    "        self.distance_at_go = self.env.get_distance()\n",
    "        self.done = True\n",
    "        self.collision, _ = self.env.simulate_go()\n",
    "        if self.collision:\n",
    "            self.reward = -10\n",
    "        else:\n",
    "            self.reward = 10 - self.penalty_per_tick * self.ticks\n",
    "\n",
    "    self.belief = self.get_belief()\n",
    "\n",
    "    return self.belief, self.reward, self.done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b439c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make and train the full observer agent.\n",
    "import go_nogo\n",
    "full_obs_agent = go_nogo.make_agent(sigma = 0, iters = 10)\n",
    "# In the output:\n",
    "# i = training iteration\n",
    "# t = number of ticks (1 tick = 0.05s)\n",
    "# r = average reward (10 is max)\n",
    "# d = average distance of the two vehicles at the time of go (in meters)\n",
    "# w = frequency of waits (agent waited the other car to pass before going)\n",
    "# c = frequency of collisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d409044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To keep training the agent without initializing it anew, use\n",
    "go_nogo.retrain_agent(full_obs_agent, iters = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e89821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's investigate if it has found a policy for the different distances.\n",
    "# Keep an eye on the \"critical\" y_start at around -13.\n",
    "HTML(go_nogo.animate_agent(full_obs_agent, y_start = -13.8, get_anim = True).to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5572776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the noisy observer\n",
    "noisy_obs_agent = go_nogo.make_agent(sigma = 0.1, iters = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c8d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To keep training the agent without initializing it anew, use\n",
    "go_nogo.retrain_agent(noisy_obs_agent, iters = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e081ef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's investigate if it has found a policy for the different distances.\n",
    "# Keep an eye on the \"critical\" y_start at around -13.\n",
    "HTML(go_nogo.animate_agent(noisy_obs_agent, y_start = -13.8, get_anim = True).to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507b68d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run an experiment for obtaining multiple samples from each agent.\n",
    "def wait_or_go_experiment(agent, y_range, n = 100, deterministic = False):\n",
    "    data = []\n",
    "    agent.env.veh1_straight_start_y_range = y_range\n",
    "    for i in range(n):\n",
    "        _, _, _, w, c = agent.run_episode(deterministic = deterministic)\n",
    "        data.append([agent.observation_var, agent.env.y_start, w, c])\n",
    "    \n",
    "    agent.env.veh1_straight_start_y_range = [-25,-2] # return back to original\n",
    "    return data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Increase n to e.g., 2000 to get more robust final image, but note that it takes longer to run.\n",
    "data = wait_or_go_experiment(full_obs_agent, y_range = [-5,-25], n = 200, deterministic = True)\n",
    "data = data + wait_or_go_experiment(noisy_obs_agent, y_range = [-5,-25], n = 200, deterministic = True)\n",
    "columns = ['sigma', 'y_start', 'wait', 'collision']\n",
    "df = pd.DataFrame(data, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6401b0",
   "metadata": {},
   "source": [
    "# 5. Inspect model validity\n",
    "\n",
    "After having converged the model to an optimal policy, our aim is to utilize it for generating simulations of task behavior. The evaluation of the model's validity encompasses multiple stages, see the workflow.pdf draft for these. Here, we concentrate solely on its face validity, which addresses whether the model aligns with our initial predictions.\n",
    "\n",
    "Starting the model's validity assessment with face validity tests is a useful practice, as any discrepancies between the model's performance and our hypotheses at this stage may indicate issues with either the model's specification or our modeling assumptions. This is frequently an iterative procedure, during which we may observe the model's divergence from our expectations, resulting in identifying inadequate definitions of objectives, task environment, or cognitive constraints.\n",
    "\n",
    "Once the model successfully demonstrates face validity, it should be subjected to a rigorous validation process, wherein its predictions are compared against human data or some alternative benchmarks. The model should e.g., generate accurate summary statistics (across a broader human population), be capable of replicating individual performance by adjusting specific parameters, and operate reasonably under changes in the environment. The specific validation depends always on the details of the modeling work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1b7231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize individual go/no go decisions between different sigma models for various y_ranges.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Note: this is a bit slow and dirty, gets slow with a lot of data. Only use for diagnosis.\n",
    "def plot_data(df):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Define custom colors and markers based on wait and collision values\n",
    "    colors = {0: 'red', 1: 'blue'}\n",
    "    markers = {0: 'o', 1: 'x'}\n",
    "\n",
    "    # Create a dictionary to store the labels we've already added to the legend\n",
    "    labels_added = {}\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        sigma = row['sigma']\n",
    "        y_start = row['y_start']\n",
    "        wait = row['wait']\n",
    "        collision = row['collision']\n",
    "        label = f'Wait: {wait}, Collision: {collision}'\n",
    "\n",
    "        # Add scatter plot point with custom color and marker\n",
    "        ax.scatter(\n",
    "            y_start,\n",
    "            sigma,\n",
    "            marker=markers[collision],\n",
    "            color=colors[wait],\n",
    "            label=label if label not in labels_added else \"\",\n",
    "            alpha=0.7\n",
    "        )\n",
    "        \n",
    "        # Remember that we've added this label to the legend\n",
    "        labels_added[label] = True\n",
    "\n",
    "    ax.set_xlabel('y_start')\n",
    "    ax.set_ylabel('sigma')\n",
    "    ax.legend(title=\"Wait, Collision\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.title('Impact of y_start on wait and collision for different sigma agents')\n",
    "    plt.show()\n",
    "    \n",
    "plot_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b80148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the probability of go/no go as the function of y_start, between different sigmas.\n",
    "\n",
    "# Change smoothness to larger to get less raggedy lines.\n",
    "# Also, increase the N in the experiment above to get more observations.\n",
    "smoothness = 1\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "def estimate_probability(df, sigma, y_start_values, window_size=1, collision = False):\n",
    "    sub_df = df[df['sigma'] == sigma].sort_values('y_start')\n",
    "    probabilities = []\n",
    "\n",
    "    for y_start in y_start_values:\n",
    "        window_df = sub_df[(sub_df['y_start'] >= y_start - window_size / 2) & (sub_df['y_start'] <= y_start + window_size / 2)]\n",
    "        probability = window_df['collision' if collision else 'wait'].sum() / len(window_df)\n",
    "        probabilities.append(probability)\n",
    "\n",
    "    return probabilities\n",
    "\n",
    "def smooth_probabilities(probabilities, sigma=1):\n",
    "    return gaussian_filter1d(probabilities, sigma=sigma)\n",
    "\n",
    "def plot_probability_lines(df, collision = False):\n",
    "    plt.style.use('seaborn-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    y_start_range = np.linspace(df['y_start'].min(), df['y_start'].max(), num=500)\n",
    "\n",
    "    for sigma in df['sigma'].unique():\n",
    "        probabilities = estimate_probability(df, sigma, y_start_range, collision = collision)\n",
    "        smoothed_probabilities = smooth_probabilities(probabilities, sigma = smoothness)\n",
    "        ax.plot(y_start_range, smoothed_probabilities, label=f'{sigma}', linewidth=2)\n",
    "\n",
    "    ax.set_xlabel('y_start', fontsize=14)\n",
    "    ax.set_ylabel('Probability of Wait', fontsize=14)\n",
    "    ax.legend(title=\"Sigma\", loc=\"upper left\", fontsize=12, title_fontsize=12)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.title('Probability of Wait across y_start by sigma', fontsize=16)\n",
    "    plt.grid(alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "plot_probability_lines(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b825f31f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
